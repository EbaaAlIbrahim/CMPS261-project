{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07352e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras import Sequential\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "034ca2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ebaa\\AppData\\Local\\Temp\\ipykernel_1528\\3515840628.py:2: DtypeWarning: Columns (8,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('HIGGS_train.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class label</th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m jj</th>\n",
       "      <th>m jjj</th>\n",
       "      <th>m lv</th>\n",
       "      <th>m jlv</th>\n",
       "      <th>m bb</th>\n",
       "      <th>m wbb</th>\n",
       "      <th>m wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.35900</td>\n",
       "      <td>1.500</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>1.100</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>2.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.140</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.799</td>\n",
       "      <td>1.470</td>\n",
       "      <td>-1.64000</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.426</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.280</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.910</td>\n",
       "      <td>1.110</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.340</td>\n",
       "      <td>-0.877</td>\n",
       "      <td>0.93600</td>\n",
       "      <td>1.990</td>\n",
       "      <td>0.882</td>\n",
       "      <td>1.790</td>\n",
       "      <td>-1.650</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678</td>\n",
       "      <td>-1.360000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.947</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.869</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.110</td>\n",
       "      <td>0.321</td>\n",
       "      <td>1.52000</td>\n",
       "      <td>0.883</td>\n",
       "      <td>-1.210</td>\n",
       "      <td>0.681</td>\n",
       "      <td>-1.070</td>\n",
       "      <td>-0.922</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.756</td>\n",
       "      <td>1.360</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.838</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.600</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>0.00707</td>\n",
       "      <td>1.820</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.848</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>-1.270000</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.75700</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>0.471</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.460</td>\n",
       "      <td>1.460000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.823</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.610</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>0.21200</td>\n",
       "      <td>0.716</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>0.553</td>\n",
       "      <td>-0.908</td>\n",
       "      <td>-1.56</td>\n",
       "      <td>2.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538</td>\n",
       "      <td>-0.490000</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.643</td>\n",
       "      <td>1.260</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.34400</td>\n",
       "      <td>0.617</td>\n",
       "      <td>-1.430</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.974</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.180</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-1.46000</td>\n",
       "      <td>0.735</td>\n",
       "      <td>-0.753</td>\n",
       "      <td>1.020</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.622000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.812</td>\n",
       "      <td>1.240</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.771</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-1.02000</td>\n",
       "      <td>1.790</td>\n",
       "      <td>-1.650</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>0.667000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.984</td>\n",
       "      <td>1.340</td>\n",
       "      <td>0.510</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>599999 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class label  lepton pT  lepton eta  lepton phi  \\\n",
       "0               1.0      0.908       0.329     0.35900   \n",
       "1               1.0      0.799       1.470    -1.64000   \n",
       "2               0.0      1.340      -0.877     0.93600   \n",
       "3               1.0      1.110       0.321     1.52000   \n",
       "4               0.0      1.600      -0.608     0.00707   \n",
       "...             ...        ...         ...         ...   \n",
       "599994          0.0      0.680       0.223    -0.75700   \n",
       "599995          1.0      1.610      -1.620     0.21200   \n",
       "599996          1.0      1.070       0.364     0.34400   \n",
       "599997          1.0      1.180      -0.173    -1.46000   \n",
       "599998          0.0      0.771      -0.133    -1.02000   \n",
       "\n",
       "        missing energy magnitude  missing energy phi  jet 1 pt  jet 1 eta  \\\n",
       "0                          1.500              -0.313     1.100     -0.558   \n",
       "1                          0.454               0.426     1.100      1.280   \n",
       "2                          1.990               0.882     1.790     -1.650   \n",
       "3                          0.883              -1.210     0.681     -1.070   \n",
       "4                          1.820              -0.112     0.848     -0.566   \n",
       "...                          ...                 ...       ...        ...   \n",
       "599994                     0.418              -0.323     0.471     -0.394   \n",
       "599995                     0.716              -0.906     0.553     -0.908   \n",
       "599996                     0.617              -1.430     0.675      0.159   \n",
       "599997                     0.735              -0.753     1.020     -0.838   \n",
       "599998                     1.790              -1.650     0.779      0.487   \n",
       "\n",
       "       jet 1 phi  jet 1 b-tag  ...  jet 4 eta  jet 4 phi  jet 4 b-tag   m jj  \\\n",
       "0          -1.59         2.17  ...     -1.140  -0.000819          0.0  0.302   \n",
       "1           1.38         0.00  ...      1.130   0.900000          0.0  0.910   \n",
       "2         -0.942         0.00  ...     -0.678  -1.360000          0.0  0.947   \n",
       "3         -0.922         0.00  ...     -0.374   0.113000          0.0  0.756   \n",
       "4           1.58         2.17  ...     -0.654  -1.270000          3.1  0.824   \n",
       "...          ...          ...  ...        ...        ...          ...    ...   \n",
       "599994     0.103         0.00  ...     -2.460   1.460000          0.0  0.823   \n",
       "599995     -1.56         2.17  ...      0.538  -0.490000          3.1  0.810   \n",
       "599996    0.0789         0.00  ...      0.978   1.150000          3.1  0.973   \n",
       "599997      1.23         0.00  ...      0.107   0.622000          0.0  0.812   \n",
       "599998      0.19         2.17  ...     -0.314   0.667000          0.0  0.829   \n",
       "\n",
       "        m jjj   m lv  m jlv   m bb  m wbb  m wwbb  \n",
       "0       0.833  0.986  0.978  0.780  0.992   0.798  \n",
       "1       1.110  0.986  0.951  0.803  0.866   0.780  \n",
       "2       1.030  0.999  0.728  0.869  1.030   0.958  \n",
       "3       1.360  0.987  0.838  1.130  0.872   0.808  \n",
       "4       0.938  0.972  0.789  0.431  0.961   0.958  \n",
       "...       ...    ...    ...    ...    ...     ...  \n",
       "599994  1.040  0.985  0.868  0.258  0.776   0.712  \n",
       "599995  0.643  1.260  1.020  0.626  0.773   0.701  \n",
       "599996  0.974  1.130  0.969  0.852  0.908   0.789  \n",
       "599997  1.240  0.986  0.694  0.745  0.741   0.728  \n",
       "599998  0.839  0.984  1.340  0.510  1.040   0.905  \n",
       "\n",
       "[599999 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# read data from the Excel file\n",
    "data = pd.read_csv('HIGGS_train.csv')\n",
    "\n",
    "# Set the column names\n",
    "column_names = ['class label', 'lepton pT', 'lepton eta', 'lepton phi', 'missing energy magnitude', \n",
    "                'missing energy phi', 'jet 1 pt', 'jet 1 eta', 'jet 1 phi', 'jet 1 b-tag', \n",
    "                'jet 2 pt', 'jet 2 eta', 'jet 2 phi', 'jet 2 b-tag', 'jet 3 pt', 'jet 3 eta', \n",
    "                'jet 3 phi', 'jet 3 b-tag', 'jet 4 pt', 'jet 4 eta', 'jet 4 phi', 'jet 4 b-tag', \n",
    "                'm jj', 'm jjj', 'm lv', 'm jlv', 'm bb', 'm wbb', 'm wwbb']\n",
    "\n",
    "data.columns = column_names\n",
    "\n",
    "# Split data into features and target\n",
    "X = data.iloc[:, 1:].values\n",
    "y = data.iloc[:, 0].values\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e897325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the column names\n",
    "column_names = ['class label', 'lepton pT', 'lepton eta', 'lepton phi', 'missing energy magnitude', \n",
    "                'missing energy phi', 'jet 1 pt', 'jet 1 eta', 'jet 1 phi', 'jet 1 b-tag', \n",
    "                'jet 2 pt', 'jet 2 eta', 'jet 2 phi', 'jet 2 b-tag', 'jet 3 pt', 'jet 3 eta', \n",
    "                'jet 3 phi', 'jet 3 b-tag', 'jet 4 pt', 'jet 4 eta', 'jet 4 phi', 'jet 4 b-tag', \n",
    "                'm jj', 'm jjj', 'm lv', 'm jlv', 'm bb', 'm wbb', 'm wwbb']\n",
    "\n",
    "data.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c105fb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class label</th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m jj</th>\n",
       "      <th>m jjj</th>\n",
       "      <th>m lv</th>\n",
       "      <th>m jlv</th>\n",
       "      <th>m bb</th>\n",
       "      <th>m wbb</th>\n",
       "      <th>m wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.35900</td>\n",
       "      <td>1.500</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>1.100</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>2.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.140</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.799</td>\n",
       "      <td>1.470</td>\n",
       "      <td>-1.64000</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.426</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.280</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.910</td>\n",
       "      <td>1.110</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.340</td>\n",
       "      <td>-0.877</td>\n",
       "      <td>0.93600</td>\n",
       "      <td>1.990</td>\n",
       "      <td>0.882</td>\n",
       "      <td>1.790</td>\n",
       "      <td>-1.650</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678</td>\n",
       "      <td>-1.360000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.947</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.869</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.110</td>\n",
       "      <td>0.321</td>\n",
       "      <td>1.52000</td>\n",
       "      <td>0.883</td>\n",
       "      <td>-1.210</td>\n",
       "      <td>0.681</td>\n",
       "      <td>-1.070</td>\n",
       "      <td>-0.922</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.756</td>\n",
       "      <td>1.360</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.838</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.600</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>0.00707</td>\n",
       "      <td>1.820</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.848</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>-1.270000</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.75700</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>0.471</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.460</td>\n",
       "      <td>1.460000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.823</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.610</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>0.21200</td>\n",
       "      <td>0.716</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>0.553</td>\n",
       "      <td>-0.908</td>\n",
       "      <td>-1.56</td>\n",
       "      <td>2.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538</td>\n",
       "      <td>-0.490000</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.643</td>\n",
       "      <td>1.260</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.34400</td>\n",
       "      <td>0.617</td>\n",
       "      <td>-1.430</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.974</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.180</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-1.46000</td>\n",
       "      <td>0.735</td>\n",
       "      <td>-0.753</td>\n",
       "      <td>1.020</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.622000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.812</td>\n",
       "      <td>1.240</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.771</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-1.02000</td>\n",
       "      <td>1.790</td>\n",
       "      <td>-1.650</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>0.667000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.984</td>\n",
       "      <td>1.340</td>\n",
       "      <td>0.510</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>599999 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class label  lepton pT  lepton eta  lepton phi  \\\n",
       "0               1.0      0.908       0.329     0.35900   \n",
       "1               1.0      0.799       1.470    -1.64000   \n",
       "2               0.0      1.340      -0.877     0.93600   \n",
       "3               1.0      1.110       0.321     1.52000   \n",
       "4               0.0      1.600      -0.608     0.00707   \n",
       "...             ...        ...         ...         ...   \n",
       "599994          0.0      0.680       0.223    -0.75700   \n",
       "599995          1.0      1.610      -1.620     0.21200   \n",
       "599996          1.0      1.070       0.364     0.34400   \n",
       "599997          1.0      1.180      -0.173    -1.46000   \n",
       "599998          0.0      0.771      -0.133    -1.02000   \n",
       "\n",
       "        missing energy magnitude  missing energy phi  jet 1 pt  jet 1 eta  \\\n",
       "0                          1.500              -0.313     1.100     -0.558   \n",
       "1                          0.454               0.426     1.100      1.280   \n",
       "2                          1.990               0.882     1.790     -1.650   \n",
       "3                          0.883              -1.210     0.681     -1.070   \n",
       "4                          1.820              -0.112     0.848     -0.566   \n",
       "...                          ...                 ...       ...        ...   \n",
       "599994                     0.418              -0.323     0.471     -0.394   \n",
       "599995                     0.716              -0.906     0.553     -0.908   \n",
       "599996                     0.617              -1.430     0.675      0.159   \n",
       "599997                     0.735              -0.753     1.020     -0.838   \n",
       "599998                     1.790              -1.650     0.779      0.487   \n",
       "\n",
       "       jet 1 phi  jet 1 b-tag  ...  jet 4 eta  jet 4 phi  jet 4 b-tag   m jj  \\\n",
       "0          -1.59         2.17  ...     -1.140  -0.000819          0.0  0.302   \n",
       "1           1.38         0.00  ...      1.130   0.900000          0.0  0.910   \n",
       "2         -0.942         0.00  ...     -0.678  -1.360000          0.0  0.947   \n",
       "3         -0.922         0.00  ...     -0.374   0.113000          0.0  0.756   \n",
       "4           1.58         2.17  ...     -0.654  -1.270000          3.1  0.824   \n",
       "...          ...          ...  ...        ...        ...          ...    ...   \n",
       "599994     0.103         0.00  ...     -2.460   1.460000          0.0  0.823   \n",
       "599995     -1.56         2.17  ...      0.538  -0.490000          3.1  0.810   \n",
       "599996    0.0789         0.00  ...      0.978   1.150000          3.1  0.973   \n",
       "599997      1.23         0.00  ...      0.107   0.622000          0.0  0.812   \n",
       "599998      0.19         2.17  ...     -0.314   0.667000          0.0  0.829   \n",
       "\n",
       "        m jjj   m lv  m jlv   m bb  m wbb  m wwbb  \n",
       "0       0.833  0.986  0.978  0.780  0.992   0.798  \n",
       "1       1.110  0.986  0.951  0.803  0.866   0.780  \n",
       "2       1.030  0.999  0.728  0.869  1.030   0.958  \n",
       "3       1.360  0.987  0.838  1.130  0.872   0.808  \n",
       "4       0.938  0.972  0.789  0.431  0.961   0.958  \n",
       "...       ...    ...    ...    ...    ...     ...  \n",
       "599994  1.040  0.985  0.868  0.258  0.776   0.712  \n",
       "599995  0.643  1.260  1.020  0.626  0.773   0.701  \n",
       "599996  0.974  1.130  0.969  0.852  0.908   0.789  \n",
       "599997  1.240  0.986  0.694  0.745  0.741   0.728  \n",
       "599998  0.839  0.984  1.340  0.510  1.040   0.905  \n",
       "\n",
       "[599999 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "004cf087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i will now remove all the data that cannot be converted into floats so that we can start implementing:\n",
    "\n",
    "# convert the values in a column to numeric format\n",
    "for i in data.columns:\n",
    "\n",
    "  data[i] = pd.to_numeric(data[i], errors='coerce') # errors will convert non convertable data to NAN\n",
    "\n",
    "# drop rows with NaN values\n",
    "data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4277fa45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class label</th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m jj</th>\n",
       "      <th>m jjj</th>\n",
       "      <th>m lv</th>\n",
       "      <th>m jlv</th>\n",
       "      <th>m bb</th>\n",
       "      <th>m wbb</th>\n",
       "      <th>m wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.35900</td>\n",
       "      <td>1.500</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>1.100</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>-1.5900</td>\n",
       "      <td>2.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.140</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.799</td>\n",
       "      <td>1.470</td>\n",
       "      <td>-1.64000</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.426</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.280</td>\n",
       "      <td>1.3800</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.910</td>\n",
       "      <td>1.110</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.340</td>\n",
       "      <td>-0.877</td>\n",
       "      <td>0.93600</td>\n",
       "      <td>1.990</td>\n",
       "      <td>0.882</td>\n",
       "      <td>1.790</td>\n",
       "      <td>-1.650</td>\n",
       "      <td>-0.9420</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678</td>\n",
       "      <td>-1.360000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.947</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.869</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.110</td>\n",
       "      <td>0.321</td>\n",
       "      <td>1.52000</td>\n",
       "      <td>0.883</td>\n",
       "      <td>-1.210</td>\n",
       "      <td>0.681</td>\n",
       "      <td>-1.070</td>\n",
       "      <td>-0.9220</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.756</td>\n",
       "      <td>1.360</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.838</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.600</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>0.00707</td>\n",
       "      <td>1.820</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.848</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>1.5800</td>\n",
       "      <td>2.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>-1.270000</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.75700</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>0.471</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.460</td>\n",
       "      <td>1.460000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.823</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.610</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>0.21200</td>\n",
       "      <td>0.716</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>0.553</td>\n",
       "      <td>-0.908</td>\n",
       "      <td>-1.5600</td>\n",
       "      <td>2.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538</td>\n",
       "      <td>-0.490000</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.643</td>\n",
       "      <td>1.260</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.34400</td>\n",
       "      <td>0.617</td>\n",
       "      <td>-1.430</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.974</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.180</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-1.46000</td>\n",
       "      <td>0.735</td>\n",
       "      <td>-0.753</td>\n",
       "      <td>1.020</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>1.2300</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.622000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.812</td>\n",
       "      <td>1.240</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.771</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-1.02000</td>\n",
       "      <td>1.790</td>\n",
       "      <td>-1.650</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>2.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>0.667000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.984</td>\n",
       "      <td>1.340</td>\n",
       "      <td>0.510</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>599995 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class label  lepton pT  lepton eta  lepton phi  \\\n",
       "0               1.0      0.908       0.329     0.35900   \n",
       "1               1.0      0.799       1.470    -1.64000   \n",
       "2               0.0      1.340      -0.877     0.93600   \n",
       "3               1.0      1.110       0.321     1.52000   \n",
       "4               0.0      1.600      -0.608     0.00707   \n",
       "...             ...        ...         ...         ...   \n",
       "599994          0.0      0.680       0.223    -0.75700   \n",
       "599995          1.0      1.610      -1.620     0.21200   \n",
       "599996          1.0      1.070       0.364     0.34400   \n",
       "599997          1.0      1.180      -0.173    -1.46000   \n",
       "599998          0.0      0.771      -0.133    -1.02000   \n",
       "\n",
       "        missing energy magnitude  missing energy phi  jet 1 pt  jet 1 eta  \\\n",
       "0                          1.500              -0.313     1.100     -0.558   \n",
       "1                          0.454               0.426     1.100      1.280   \n",
       "2                          1.990               0.882     1.790     -1.650   \n",
       "3                          0.883              -1.210     0.681     -1.070   \n",
       "4                          1.820              -0.112     0.848     -0.566   \n",
       "...                          ...                 ...       ...        ...   \n",
       "599994                     0.418              -0.323     0.471     -0.394   \n",
       "599995                     0.716              -0.906     0.553     -0.908   \n",
       "599996                     0.617              -1.430     0.675      0.159   \n",
       "599997                     0.735              -0.753     1.020     -0.838   \n",
       "599998                     1.790              -1.650     0.779      0.487   \n",
       "\n",
       "        jet 1 phi  jet 1 b-tag  ...  jet 4 eta  jet 4 phi  jet 4 b-tag   m jj  \\\n",
       "0         -1.5900         2.17  ...     -1.140  -0.000819          0.0  0.302   \n",
       "1          1.3800         0.00  ...      1.130   0.900000          0.0  0.910   \n",
       "2         -0.9420         0.00  ...     -0.678  -1.360000          0.0  0.947   \n",
       "3         -0.9220         0.00  ...     -0.374   0.113000          0.0  0.756   \n",
       "4          1.5800         2.17  ...     -0.654  -1.270000          3.1  0.824   \n",
       "...           ...          ...  ...        ...        ...          ...    ...   \n",
       "599994     0.1030         0.00  ...     -2.460   1.460000          0.0  0.823   \n",
       "599995    -1.5600         2.17  ...      0.538  -0.490000          3.1  0.810   \n",
       "599996     0.0789         0.00  ...      0.978   1.150000          3.1  0.973   \n",
       "599997     1.2300         0.00  ...      0.107   0.622000          0.0  0.812   \n",
       "599998     0.1900         2.17  ...     -0.314   0.667000          0.0  0.829   \n",
       "\n",
       "        m jjj   m lv  m jlv   m bb  m wbb  m wwbb  \n",
       "0       0.833  0.986  0.978  0.780  0.992   0.798  \n",
       "1       1.110  0.986  0.951  0.803  0.866   0.780  \n",
       "2       1.030  0.999  0.728  0.869  1.030   0.958  \n",
       "3       1.360  0.987  0.838  1.130  0.872   0.808  \n",
       "4       0.938  0.972  0.789  0.431  0.961   0.958  \n",
       "...       ...    ...    ...    ...    ...     ...  \n",
       "599994  1.040  0.985  0.868  0.258  0.776   0.712  \n",
       "599995  0.643  1.260  1.020  0.626  0.773   0.701  \n",
       "599996  0.974  1.130  0.969  0.852  0.908   0.789  \n",
       "599997  1.240  0.986  0.694  0.745  0.741   0.728  \n",
       "599998  0.839  0.984  1.340  0.510  1.040   0.905  \n",
       "\n",
       "[599995 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aefed45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy of model is: 0.7609250910424253\n",
      "testing accuracy of model is: 0.7500979174826456\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state = 45)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model_1 =  MLPClassifier(hidden_layer_sizes=(50,55,45,40),activation='relu',solver='adam',alpha=0.03,random_state = 45)\n",
    "model_1.fit(X_train,y_train)\n",
    "model_1_predict=model_1.predict(X_test)\n",
    "model_1_predict_train=model_1.predict(X_train)\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_1_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc8b6f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy of model is: 0.7592021600180001\n",
      "testing accuracy of model is: 0.7473062275518962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state = 45)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model_1 =  MLPClassifier(hidden_layer_sizes=(60,50),activation='relu',solver='adam',alpha=0.001,random_state = 45)\n",
    "model_1.fit(X_train,y_train)\n",
    "model_1_predict=model_1.predict(X_test)\n",
    "model_1_predict_train=model_1.predict(X_train)\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_1_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf2d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state = 45)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model_1 =  MLPClassifier(hidden_layer_sizes=(70,65,60),activation='relu',solver='adam',alpha=.03,random_state  =45)\n",
    "model_1.fit(X_train,y_train)\n",
    "model_1_predict=model_1.predict(X_test)\n",
    "model_1_predict_train=model_1.predict(X_train)\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_1_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd0cbe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ebaa\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy of model is: 0.7397582479853999\n",
      "testing accuracy of model is: 0.7325894382453187\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state = 45)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model_1 =  MLPClassifier(hidden_layer_sizes=(1000,500,250,275,100),activation='relu',solver='adam',alpha=.01,random_state = 45)\n",
    "model_1.fit(X_train,y_train)\n",
    "model_1_predict=model_1.predict(X_test)\n",
    "model_1_predict_train=model_1.predict(X_train)\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_1_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d5ac1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy of model is: 0.7739564496370803\n",
      "testing accuracy of model is: 0.752489604080034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state = 45)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model_1 =  MLPClassifier(hidden_layer_sizes=(300,150,165,135),activation='relu',solver='adam',alpha=.03,random_state = 45)\n",
    "model_1.fit(X_train,y_train)\n",
    "model_1_predict=model_1.predict(X_test)\n",
    "model_1_predict_train=model_1.predict(X_train)\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_1_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e48453b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ebaa\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy of model is: 0.7085496545804548\n",
      "testing accuracy of model is: 0.7065725547712898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state = 7651)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model_1 =  MLPClassifier(hidden_layer_sizes=(30,50,70,100),activation='relu',solver='adam',alpha=.01,random_state = 7651)\n",
    "model_1.fit(X_train,y_train)\n",
    "model_1_predict=model_1.predict(X_test)\n",
    "model_1_predict_train=model_1.predict(X_train)\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_1_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6198f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy of model is: 0.7618625988549904\n",
      "testing accuracy of model is: 0.7532312769273077\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state = 7651)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model_1 =  MLPClassifier(hidden_layer_sizes=(50,55,45,30),activation='relu',solver='adam',alpha=.03,random_state = 7651)\n",
    "model_1.fit(X_train,y_train)\n",
    "model_1_predict=model_1.predict(X_test)\n",
    "model_1_predict_train=model_1.predict(X_train)\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_1_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b52bd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy of model is: 0.7467478895657463\n",
      "testing accuracy of model is: 0.7422561854682123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state = 7651)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model_1 =  MLPClassifier(hidden_layer_sizes=(30,20,10,5),activation='relu',solver='adam',alpha=.01,random_state = 7651)\n",
    "model_1.fit(X_train,y_train)\n",
    "model_1_predict=model_1.predict(X_test)\n",
    "model_1_predict_train=model_1.predict(X_train)\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_1_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90c25e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ebaa\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy of model is: 0.8812552604605038\n",
      "testing accuracy of model is: 0.7161809681747348\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state = 0)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model_1 =  MLPClassifier(hidden_layer_sizes=(350,300,240,245,230,200),activation='relu',solver='adam',alpha=.001,random_state = 0)\n",
    "model_1.fit(X_train,y_train)\n",
    "model_1_predict=model_1.predict(X_test)\n",
    "model_1_predict_train=model_1.predict(X_train)\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_1_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19d6bc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ebaa\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy of model is: 0.7581709014241785\n",
      "testing accuracy of model is: 0.7381394844957041\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state = 42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model_1 =  MLPClassifier(hidden_layer_sizes=(150,100,50,55,45,30),activation='relu',batch_size=100000,solver='adam',alpha=.01,random_state = 42)\n",
    "model_1.fit(X_train,y_train)\n",
    "model_1_predict=model_1.predict(X_test)\n",
    "model_1_predict_train=model_1.predict(X_train)\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_1_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "533081c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ebaa\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy of model is: 0.7671459762164685\n",
      "testing accuracy of model is: 0.7411728431070259\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state = 42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model_1 =  MLPClassifier(hidden_layer_sizes=(150,100,50,55,45,30),activation='relu',batch_size=50000,solver='adam',alpha=.01,random_state = 42)\n",
    "model_1.fit(X_train,y_train)\n",
    "model_1_predict=model_1.predict(X_test)\n",
    "model_1_predict_train=model_1.predict(X_train)\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_1_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8fa2b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ebaa\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy of model is: 0.773785614880124\n",
      "testing accuracy of model is: 0.7366728056067134\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state = 42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model_1 =  MLPClassifier(hidden_layer_sizes=(200,100,50,55,45,30),activation='relu',batch_size=50000,solver='adam',alpha=.01,random_state = 42)\n",
    "model_1.fit(X_train,y_train)\n",
    "model_1_predict=model_1.predict(X_test)\n",
    "model_1_predict_train=model_1.predict(X_train)\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_1_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8abef672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ebaa\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy of model is: 0.7860669672247269\n",
      "testing accuracy of model is: 0.7370394753289611\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state = 7651)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model_1 =  MLPClassifier(hidden_layer_sizes=(100,80,50,55,45,30),activation='relu',solver='adam',alpha=.03,random_state = 7651)\n",
    "model_1.fit(X_train,y_train)\n",
    "model_1_predict=model_1.predict(X_test)\n",
    "model_1_predict_train=model_1.predict(X_train)\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_1_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4fb7c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy of model is: 0.7666376386469888\n",
      "testing accuracy of model is: 0.7550729589413245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state = 42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model_1 =  MLPClassifier(hidden_layer_sizes=(100,50,55,45,30),activation='relu',solver='adam',alpha=.03,random_state = 42)\n",
    "model_1.fit(X_train,y_train)\n",
    "model_1_predict=model_1.predict(X_test)\n",
    "model_1_predict_train=model_1.predict(X_train)\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_1_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb99ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state = 7651)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model_1 =  MLPClassifier(hidden_layer_sizes=(150,100,50,55,45,30),activation='relu',batch_size=10000,solver='adam',alpha=.03,random_state = 7651)\n",
    "model_1.fit(X_train,y_train)\n",
    "model_1_predict=model_1.predict(X_test)\n",
    "model_1_predict_train=model_1.predict(X_train)\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_1_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_1_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "742a134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "15000/15000 [==============================] - 116s 7ms/step - loss: 0.6249 - accuracy: 0.6566\n",
      "Epoch 2/12\n",
      "15000/15000 [==============================] - 130s 9ms/step - loss: 0.5897 - accuracy: 0.6887\n",
      "Epoch 3/12\n",
      "15000/15000 [==============================] - 97s 6ms/step - loss: 0.5759 - accuracy: 0.7015\n",
      "Epoch 4/12\n",
      "15000/15000 [==============================] - 86s 6ms/step - loss: 0.5642 - accuracy: 0.7105\n",
      "Epoch 5/12\n",
      "15000/15000 [==============================] - 78s 5ms/step - loss: 0.5579 - accuracy: 0.7153\n",
      "Epoch 6/12\n",
      "15000/15000 [==============================] - 108s 7ms/step - loss: 0.5589 - accuracy: 0.7178\n",
      "Epoch 7/12\n",
      "15000/15000 [==============================] - 80s 5ms/step - loss: 0.5475 - accuracy: 0.7245\n",
      "Epoch 8/12\n",
      "15000/15000 [==============================] - 74s 5ms/step - loss: 0.5395 - accuracy: 0.7279\n",
      "Epoch 9/12\n",
      "15000/15000 [==============================] - 79s 5ms/step - loss: 0.5348 - accuracy: 0.7320\n",
      "Epoch 10/12\n",
      "15000/15000 [==============================] - 85s 6ms/step - loss: 0.5359 - accuracy: 0.7329\n",
      "Epoch 11/12\n",
      "15000/15000 [==============================] - 72s 5ms/step - loss: 0.5351 - accuracy: 0.7349\n",
      "Epoch 12/12\n",
      "15000/15000 [==============================] - 76s 5ms/step - loss: 0.5366 - accuracy: 0.7364\n",
      "training accuracy of model is: 0.7325769381411512\n",
      "testing accuracy of model is: 0.7249643747031225\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from keras import Sequential\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state=1)\n",
    "model = Sequential([\n",
    "Dense(units=350, activation='relu'),\n",
    "Dense(units=345, activation='relu'),\n",
    "Dense(units=340, activation='relu'),\n",
    "Dense(units=300, activation='relu'),\n",
    "Dense(units=250, activation='relu'),\n",
    "Dense(units=200, activation='relu'),\n",
    "Dense(units=100, activation='relu'),\n",
    "Dense(units=50, activation='relu'),\n",
    "Dense(units=1, activation='relu'),\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, epochs=12)\n",
    "model_predict=np.round(model.predict(X_test))\n",
    "model_predict_train=np.round(model.predict(X_train))\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c64741f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.6549 - accuracy: 0.6168\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.6077 - accuracy: 0.6686\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.6000 - accuracy: 0.6733\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5797 - accuracy: 0.6941\n",
      "Epoch 5/100\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.6048 - accuracy: 0.6693\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.5766 - accuracy: 0.6981\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5546 - accuracy: 0.7137\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.5519 - accuracy: 0.7164\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.5624 - accuracy: 0.7008\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.5386 - accuracy: 0.7260\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.5282 - accuracy: 0.7324\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5262 - accuracy: 0.7352\n",
      "Epoch 13/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5175 - accuracy: 0.7404\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.5126 - accuracy: 0.7440\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5081 - accuracy: 0.7476\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5260 - accuracy: 0.7382\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5121 - accuracy: 0.7458\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4982 - accuracy: 0.7542\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.4967 - accuracy: 0.7545\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.4960 - accuracy: 0.7568\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.4876 - accuracy: 0.7614\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.4992 - accuracy: 0.7543\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5135 - accuracy: 0.7455\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.4946 - accuracy: 0.7576\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.4922 - accuracy: 0.7604\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5116 - accuracy: 0.7498\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4872 - accuracy: 0.7634\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.4764 - accuracy: 0.7702\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.4687 - accuracy: 0.7738\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4642 - accuracy: 0.7763\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.4987 - accuracy: 0.7543\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4933 - accuracy: 0.7593\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.5192 - accuracy: 0.7424\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.5027 - accuracy: 0.7529\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4870 - accuracy: 0.7632\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4916 - accuracy: 0.7602\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.4976 - accuracy: 0.7574\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.5083 - accuracy: 0.7522\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.4903 - accuracy: 0.7630\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.4983 - accuracy: 0.7596\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4732 - accuracy: 0.7736\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.4666 - accuracy: 0.7781\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.4672 - accuracy: 0.7786\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4943 - accuracy: 0.7598\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.4608 - accuracy: 0.7810\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.4479 - accuracy: 0.7884\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.4568 - accuracy: 0.7836\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.4636 - accuracy: 0.7793\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4363 - accuracy: 0.7962\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.4389 - accuracy: 0.7944\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4260 - accuracy: 0.8020\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4245 - accuracy: 0.8027\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.4403 - accuracy: 0.7942\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4214 - accuracy: 0.8045\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4263 - accuracy: 0.8014\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.4234 - accuracy: 0.8037\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4247 - accuracy: 0.8047\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4055 - accuracy: 0.8135\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.3991 - accuracy: 0.8167\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4019 - accuracy: 0.8150\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.3895 - accuracy: 0.8223\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.3759 - accuracy: 0.8299\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.3699 - accuracy: 0.8335\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.3655 - accuracy: 0.8362\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.3874 - accuracy: 0.8235\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.3939 - accuracy: 0.8203\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.3719 - accuracy: 0.8336\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.3604 - accuracy: 0.8385\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.3528 - accuracy: 0.8423\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.3591 - accuracy: 0.8385\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.3831 - accuracy: 0.8303\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4679 - accuracy: 0.7788\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.4156 - accuracy: 0.8087\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5259 - accuracy: 0.7415\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4978 - accuracy: 0.7565\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.4474 - accuracy: 0.7894\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4265 - accuracy: 0.8016\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4220 - accuracy: 0.8038\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4055 - accuracy: 0.8132\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.3758 - accuracy: 0.8305\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4035 - accuracy: 0.8172\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4393 - accuracy: 0.7911\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.3944 - accuracy: 0.8197\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.3600 - accuracy: 0.8384\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.4304 - accuracy: 0.8039\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.4524 - accuracy: 0.7862\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4610 - accuracy: 0.7829\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4158 - accuracy: 0.8082\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.3900 - accuracy: 0.8228\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4051 - accuracy: 0.8134\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.3990 - accuracy: 0.8184\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.3866 - accuracy: 0.8290\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.3744 - accuracy: 0.8328\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4461 - accuracy: 0.7905\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4597 - accuracy: 0.7819\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4262 - accuracy: 0.8012\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4944 - accuracy: 0.7600\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4314 - accuracy: 0.7991\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4099 - accuracy: 0.8125\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4280 - accuracy: 0.8013\n",
      "training accuracy of model is: 0.7973712280935674\n",
      "testing accuracy of model is: 0.701647513729281\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from keras import Sequential\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state=7651)\n",
    "model = Sequential([\n",
    "Dense(units=500, activation='relu'),\n",
    "Dense(units=400, activation='relu'),\n",
    "Dense(units=300, activation='relu'),\n",
    "Dense(units=200, activation='relu'),\n",
    "Dense(units=150, activation='relu'),\n",
    "Dense(units=100, activation='relu'),\n",
    "Dense(units=50, activation='relu'),\n",
    "Dense(units=45, activation='relu'),\n",
    "Dense(units=30, activation='relu'),\n",
    "Dense(units=1, activation='relu'),\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, epochs=100,steps_per_epoch=500)\n",
    "model_predict=np.round(model.predict(X_test))\n",
    "model_predict_train=np.round(model.predict(X_train))\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "708fee32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6545 - accuracy: 0.6234\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6033 - accuracy: 0.6733\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6022 - accuracy: 0.6725\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.5712 - accuracy: 0.7022\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5753 - accuracy: 0.7024\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.5668 - accuracy: 0.7063\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5478 - accuracy: 0.7206\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.5448 - accuracy: 0.7235\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5335 - accuracy: 0.7308\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.5229 - accuracy: 0.7378\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.5236 - accuracy: 0.7387\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.5106 - accuracy: 0.7461\n",
      "training accuracy of model is: 0.7516104300869174\n",
      "testing accuracy of model is: 0.7342311185926549\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from keras import Sequential\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state=7651)\n",
    "model = Sequential([\n",
    "Dense(units=500, activation='relu'),\n",
    "Dense(units=400, activation='relu'),\n",
    "Dense(units=300, activation='relu'),\n",
    "Dense(units=200, activation='relu'),\n",
    "Dense(units=150, activation='relu'),\n",
    "Dense(units=100, activation='relu'),\n",
    "Dense(units=50, activation='relu'),\n",
    "Dense(units=45, activation='relu'),\n",
    "Dense(units=30, activation='relu'),\n",
    "Dense(units=1, activation='relu'),\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, epochs=12,steps_per_epoch=1000)\n",
    "model_predict=np.round(model.predict(X_test))\n",
    "model_predict_train=np.round(model.predict(X_train))\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "723012f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6617 - accuracy: 0.6148\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6050 - accuracy: 0.6689\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6001 - accuracy: 0.6731\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5872 - accuracy: 0.6865\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5640 - accuracy: 0.7063\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.5828 - accuracy: 0.6927\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5637 - accuracy: 0.7091\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.5550 - accuracy: 0.7146\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5621 - accuracy: 0.7085\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.5472 - accuracy: 0.7199\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5402 - accuracy: 0.7261\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.5304 - accuracy: 0.7317\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.5371 - accuracy: 0.7277\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.5309 - accuracy: 0.7315\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.5200 - accuracy: 0.7393\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.5197 - accuracy: 0.7396\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.5095 - accuracy: 0.7467\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5275 - accuracy: 0.7349\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.5281 - accuracy: 0.7348\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5175 - accuracy: 0.7429\n",
      "training accuracy of model is: 0.753285444045367\n",
      "testing accuracy of model is: 0.7375311460928841\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from keras import Sequential\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state=7651)\n",
    "model = Sequential([\n",
    "Dense(units=500, activation='relu'),\n",
    "Dense(units=400, activation='relu'),\n",
    "Dense(units=300, activation='relu'),\n",
    "Dense(units=200, activation='relu'),\n",
    "Dense(units=150, activation='relu'),\n",
    "Dense(units=100, activation='relu'),\n",
    "Dense(units=50, activation='relu'),\n",
    "Dense(units=45, activation='relu'),\n",
    "Dense(units=30, activation='relu'),\n",
    "Dense(units=1, activation='relu'),\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, epochs=20,steps_per_epoch=1000)\n",
    "model_predict=np.round(model.predict(X_test))\n",
    "model_predict_train=np.round(model.predict(X_train))\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a244751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "5000/5000 [==============================] - 26s 5ms/step - loss: 0.6419 - accuracy: 0.6335\n",
      "Epoch 2/12\n",
      "5000/5000 [==============================] - 27s 5ms/step - loss: 0.5936 - accuracy: 0.6807\n",
      "Epoch 3/12\n",
      "5000/5000 [==============================] - 28s 6ms/step - loss: 0.5722 - accuracy: 0.7012\n",
      "Epoch 4/12\n",
      "5000/5000 [==============================] - 28s 6ms/step - loss: 0.5595 - accuracy: 0.7111\n",
      "Epoch 5/12\n",
      "5000/5000 [==============================] - 31s 6ms/step - loss: 0.5466 - accuracy: 0.7207\n",
      "Epoch 6/12\n",
      "5000/5000 [==============================] - 31s 6ms/step - loss: 0.5399 - accuracy: 0.7265\n",
      "Epoch 7/12\n",
      "5000/5000 [==============================] - 35s 7ms/step - loss: 0.5431 - accuracy: 0.7261\n",
      "Epoch 8/12\n",
      "5000/5000 [==============================] - 38s 8ms/step - loss: 0.5493 - accuracy: 0.7236\n",
      "Epoch 9/12\n",
      "5000/5000 [==============================] - 41s 8ms/step - loss: 0.5357 - accuracy: 0.7328\n",
      "Epoch 10/12\n",
      "5000/5000 [==============================] - 35s 7ms/step - loss: 0.5282 - accuracy: 0.7352\n",
      "Epoch 11/12\n",
      "5000/5000 [==============================] - 28s 6ms/step - loss: 0.5313 - accuracy: 0.7365\n",
      "Epoch 12/12\n",
      "5000/5000 [==============================] - 28s 6ms/step - loss: 0.5240 - accuracy: 0.7385\n",
      "training accuracy of model is: 0.7421186843223694\n",
      "testing accuracy of model is: 0.7302060850507087\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from keras import Sequential\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state=7651)\n",
    "model = Sequential([\n",
    "Dense(units=500, activation='relu'),\n",
    "Dense(units=400, activation='relu'),\n",
    "Dense(units=300, activation='relu'),\n",
    "Dense(units=200, activation='relu'),\n",
    "Dense(units=150, activation='relu'),\n",
    "Dense(units=100, activation='relu'),\n",
    "Dense(units=50, activation='relu'),\n",
    "Dense(units=45, activation='relu'),\n",
    "Dense(units=30, activation='relu'),\n",
    "Dense(units=1, activation='relu'),\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, epochs=12,steps_per_epoch=5000)\n",
    "model_predict=np.round(model.predict(X_test))\n",
    "model_predict_train=np.round(model.predict(X_train))\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bad3d3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "500/500 [==============================] - 6s 9ms/step - loss: 0.6897 - accuracy: 0.5886\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.6546 - accuracy: 0.6255\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.6155 - accuracy: 0.6601\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.6066 - accuracy: 0.6689\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5925 - accuracy: 0.6829\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.6265 - accuracy: 0.6500\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5848 - accuracy: 0.6900\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.5747 - accuracy: 0.6986\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.6081 - accuracy: 0.6751\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.6120 - accuracy: 0.6701\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5797 - accuracy: 0.6953\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5721 - accuracy: 0.7005\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5729 - accuracy: 0.6987\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5770 - accuracy: 0.6958\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.6456 - accuracy: 0.6396\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5847 - accuracy: 0.6913\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5752 - accuracy: 0.6977\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5827 - accuracy: 0.6866\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.6012 - accuracy: 0.6772\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5767 - accuracy: 0.6995\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5996 - accuracy: 0.6797\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.6101 - accuracy: 0.6744\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5731 - accuracy: 0.7009\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5631 - accuracy: 0.7092\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5700 - accuracy: 0.7007\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5518 - accuracy: 0.7184\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5442 - accuracy: 0.7228\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5417 - accuracy: 0.7250\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5331 - accuracy: 0.7294\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5348 - accuracy: 0.7290\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5330 - accuracy: 0.7310\n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5213 - accuracy: 0.7374\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5366 - accuracy: 0.7281\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5700 - accuracy: 0.7019\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5426 - accuracy: 0.7265\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5287 - accuracy: 0.7349\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5353 - accuracy: 0.7307\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5327 - accuracy: 0.7334\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5244 - accuracy: 0.7391\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5234 - accuracy: 0.7404\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5109 - accuracy: 0.7461\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5086 - accuracy: 0.7487\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5159 - accuracy: 0.7424\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5174 - accuracy: 0.7444\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5178 - accuracy: 0.7431\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5042 - accuracy: 0.7509\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5168 - accuracy: 0.7445\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5299 - accuracy: 0.7392\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5062 - accuracy: 0.7510\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.5039 - accuracy: 0.7534\n",
      "training accuracy of model is: 0.7394040783673197\n",
      "testing accuracy of model is: 0.7206560054667123\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from keras import Sequential\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state=7651)\n",
    "model = Sequential([\n",
    "Dense(units=500, activation='relu'),\n",
    "Dense(units=400, activation='relu'),\n",
    "Dense(units=300, activation='relu'),\n",
    "Dense(units=200, activation='relu'),\n",
    "Dense(units=150, activation='relu'),\n",
    "Dense(units=100, activation='relu'),\n",
    "Dense(units=50, activation='relu'),\n",
    "Dense(units=45, activation='relu'),\n",
    "Dense(units=30, activation='relu'),\n",
    "Dense(units=1, activation='relu'),\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, epochs=10,steps_per_epoch=20000)\n",
    "model_predict=np.round(model.predict(X_test))\n",
    "model_predict_train=np.round(model.predict(X_train))\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ae6af5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 90s 4ms/step - loss: 0.6229 - accuracy: 0.6517\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 74s 4ms/step - loss: 0.5920 - accuracy: 0.6843\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 80s 4ms/step - loss: 0.5884 - accuracy: 0.6904\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 98s 5ms/step - loss: 0.5773 - accuracy: 0.7003\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 104s 5ms/step - loss: 0.5718 - accuracy: 0.7046\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 88s 4ms/step - loss: 0.5673 - accuracy: 0.7089\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 90s 4ms/step - loss: 0.5685 - accuracy: 0.7104\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 109s 5ms/step - loss: 0.5656 - accuracy: 0.7124\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 109s 5ms/step - loss: 0.5606 - accuracy: 0.7150\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 108s 5ms/step - loss: 0.5578 - accuracy: 0.7171\n",
      "training accuracy of model is: 0.7158392986608222\n",
      "testing accuracy of model is: 0.7134226118550988\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from keras import Sequential\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state=7651)\n",
    "model = Sequential([\n",
    "Dense(units=100, activation='relu'),\n",
    "Dense(units=50, activation='relu'),\n",
    "Dense(units=55, activation='relu'),\n",
    "Dense(units=45, activation='relu'),\n",
    "Dense(units=30, activation='relu'),\n",
    "Dense(units=1, activation='relu'),\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, epochs=10,steps_per_epoch=20000)\n",
    "model_predict=np.round(model.predict(X_test))\n",
    "model_predict_train=np.round(model.predict(X_train))\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e50d2e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 66s 4ms/step - loss: 0.6218 - accuracy: 0.6523\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 82s 5ms/step - loss: 0.5856 - accuracy: 0.6893\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 58s 4ms/step - loss: 0.5808 - accuracy: 0.6957\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 58s 4ms/step - loss: 0.5694 - accuracy: 0.7046\n",
      "Epoch 5/20\n",
      "12632/15000 [========================>.....] - ETA: 9s - loss: 0.5613 - accuracy: 0.7114"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from keras import Sequential\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state=7651)\n",
    "model = Sequential([\n",
    "Dense(units=60, activation='relu'),\n",
    "Dense(units=50, activation='relu'),\n",
    "Dense(units=40, activation='relu'),\n",
    "Dense(units=20, activation='relu'),\n",
    "Dense(units=1, activation='relu'),\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, epochs=20)\n",
    "model_predict=np.round(model.predict(X_test))\n",
    "model_predict_train=np.round(model.predict(X_train))\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "505f0b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  400/15000 [..............................] - ETA: 4:55 - loss: 8.1535 - accuracy: 0.4714"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m model_predict\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mround(model\u001b[38;5;241m.\u001b[39mpredict(X_test))\n\u001b[0;32m     16\u001b[0m model_predict_train\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mround(model\u001b[38;5;241m.\u001b[39mpredict(X_train))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:,0],test_size=0.2,random_state=7651)\n",
    "model = Sequential([\n",
    "Dense(units=60, activation='relu'),\n",
    "Dense(units=50, activation='relu'),\n",
    "Dense(units=40, activation='relu'),\n",
    "Dense(units=20, activation='relu'),\n",
    "Dense(units=1, activation='relu'),\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, epochs=20)\n",
    "model_predict=np.round(model.predict(X_test))\n",
    "model_predict_train=np.round(model.predict(X_train))\n",
    "print('training accuracy of model is:',accuracy_score(y_train,model_predict_train))\n",
    "print('testing accuracy of model is:',accuracy_score(y_test,model_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ac545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
